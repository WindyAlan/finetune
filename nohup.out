Some weights of the model checkpoint at /home/hanzhang/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/hanzhang/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/hanzhang/miniconda3/envs/mu13/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
epoch:  0 loss:  0.6861569285392761
Validation Accuracy: 0.49819711538461536
epoch:  0 loss:  0.704055666923523
epoch:  0 loss:  0.6565402746200562
epoch:  0 loss:  0.6346715092658997
epoch:  0 loss:  0.5918811559677124
epoch:  0 loss:  0.6176143884658813
epoch:  0 loss:  0.5678094625473022
epoch:  0 loss:  0.4247932732105255
epoch:  0 loss:  0.5068386197090149
epoch:  0 loss:  0.40182316303253174
epoch:  0 loss:  0.3320322036743164
epoch:  0 loss:  0.3080051839351654
epoch:  0 loss:  0.35968610644340515
epoch:  0 loss:  0.37942934036254883
epoch:  0 loss:  0.3329111933708191
epoch:  0 loss:  0.20659860968589783
epoch:  0 loss:  0.30299338698387146
epoch:  0 loss:  0.3325639069080353
epoch:  0 loss:  0.5436078906059265
epoch:  0 loss:  0.20249757170677185
epoch:  0 loss:  0.42293456196784973
epoch:  0 loss:  0.3296194076538086
epoch:  0 loss:  0.2412482500076294
epoch:  0 loss:  0.25891783833503723
epoch:  0 loss:  0.19455444812774658
epoch:  0 loss:  0.20675913989543915
epoch:  0 loss:  0.20946529507637024
epoch:  0 loss:  0.18421900272369385
epoch:  0 loss:  0.23560142517089844
epoch:  0 loss:  0.21100912988185883
epoch:  0 loss:  0.22908169031143188
epoch:  0 loss:  0.20169800519943237
epoch:  0 loss:  0.15624971687793732
epoch:  0 loss:  0.2418421357870102
epoch:  0 loss:  0.09156949818134308
epoch:  0 loss:  0.2340625375509262
epoch:  0 loss:  0.12574052810668945
epoch:  0 loss:  0.2434598207473755
epoch:  0 loss:  0.2220659852027893
epoch:  0 loss:  0.12493301182985306
epoch:  0 loss:  0.318706750869751
epoch:  0 loss:  0.25564029812812805
epoch:  0 loss:  0.12778720259666443
epoch:  0 loss:  0.32354211807250977
epoch:  0 loss:  0.23282663524150848
epoch:  0 loss:  0.2297426164150238
epoch:  0 loss:  0.22810029983520508
epoch:  0 loss:  0.27528855204582214
epoch:  0 loss:  0.1935712993144989
epoch:  0 loss:  0.17794306576251984
epoch:  0 loss:  0.1938464641571045
epoch:  0 loss:  0.18300685286521912
epoch:  0 loss:  0.16112884879112244
epoch:  0 loss:  0.11524181067943573
epoch:  0 loss:  0.3278197646141052
epoch:  0 loss:  0.24309030175209045
epoch:  0 loss:  0.08740339428186417
epoch:  0 loss:  0.20024430751800537
epoch:  0 loss:  0.1842317283153534
epoch:  0 loss:  0.17106160521507263
epoch:  0 loss:  0.16945408284664154
epoch:  0 loss:  0.29441383481025696
epoch:  0 loss:  0.13081218302249908
epoch:  1 loss:  0.11008185893297195
Validation Accuracy: 0.9429086538461539
epoch:  1 loss:  0.046592965722084045
epoch:  1 loss:  0.1524687260389328
epoch:  1 loss:  0.1116107702255249
epoch:  1 loss:  0.10987334698438644
epoch:  1 loss:  0.0271224956959486
epoch:  1 loss:  0.06834904104471207
epoch:  1 loss:  0.05593487247824669
epoch:  1 loss:  0.08861740678548813
epoch:  1 loss:  0.0312565341591835
epoch:  1 loss:  0.03244767338037491
epoch:  1 loss:  0.04036236181855202
epoch:  1 loss:  0.010323561728000641
epoch:  1 loss:  0.058760497719049454
epoch:  1 loss:  0.1476413607597351
epoch:  1 loss:  0.03875098004937172
epoch:  1 loss:  0.07534679770469666
epoch:  1 loss:  0.10794280469417572
epoch:  1 loss:  0.22062239050865173
epoch:  1 loss:  0.024604465812444687
epoch:  1 loss:  0.08610721677541733
epoch:  1 loss:  0.11674878001213074
epoch:  1 loss:  0.08037514984607697
epoch:  1 loss:  0.019204208627343178
epoch:  1 loss:  0.03395400941371918
epoch:  1 loss:  0.07256607711315155
epoch:  1 loss:  0.23794516921043396
epoch:  1 loss:  0.014735923148691654
epoch:  1 loss:  0.12895438075065613
epoch:  1 loss:  0.10081522166728973
epoch:  1 loss:  0.07841221988201141
epoch:  1 loss:  0.05689474567770958
epoch:  1 loss:  0.11073819547891617
epoch:  1 loss:  0.0730048269033432
epoch:  1 loss:  0.032348424196243286
epoch:  1 loss:  0.16379877924919128
epoch:  1 loss:  0.04605269432067871
epoch:  1 loss:  0.06991632282733917
epoch:  1 loss:  0.07030322402715683
epoch:  1 loss:  0.028928957879543304
epoch:  1 loss:  0.04424254596233368
epoch:  1 loss:  0.07928668707609177
epoch:  1 loss:  0.054457537829875946
epoch:  1 loss:  0.1574886292219162
epoch:  1 loss:  0.12619024515151978
epoch:  1 loss:  0.07686792314052582
epoch:  1 loss:  0.07659862190485
epoch:  1 loss:  0.01758243702352047
epoch:  1 loss:  0.03084777668118477
epoch:  1 loss:  0.04935036599636078
epoch:  1 loss:  0.06946983933448792
epoch:  1 loss:  0.11144860088825226
epoch:  1 loss:  0.0707743689417839
epoch:  1 loss:  0.12070794403553009
epoch:  1 loss:  0.06156676262617111
epoch:  1 loss:  0.22459788620471954
epoch:  1 loss:  0.1524748057126999
epoch:  1 loss:  0.021077673882246017
epoch:  1 loss:  0.04297441989183426
epoch:  1 loss:  0.15459062159061432
epoch:  1 loss:  0.06965404003858566
epoch:  1 loss:  0.1117267832159996
epoch:  1 loss:  0.023137353360652924
epoch:  2 loss:  0.011766273528337479
Validation Accuracy: 0.9226262019230769
epoch:  2 loss:  0.050400614738464355
epoch:  2 loss:  0.02085312269628048
epoch:  2 loss:  0.03737910836935043
epoch:  2 loss:  0.06243234500288963
epoch:  2 loss:  0.015549568459391594
epoch:  2 loss:  0.010042660869657993
epoch:  2 loss:  0.014955989085137844
epoch:  2 loss:  0.03771013021469116
epoch:  2 loss:  0.023857148364186287
epoch:  2 loss:  0.006002375390380621
epoch:  2 loss:  0.0684320256114006
epoch:  2 loss:  0.030834130942821503
epoch:  2 loss:  0.00360351474955678
epoch:  2 loss:  0.007233311887830496
epoch:  2 loss:  0.0053122262470424175
epoch:  2 loss:  0.0837896466255188
epoch:  2 loss:  0.007010876201093197
epoch:  2 loss:  0.005415122956037521
epoch:  2 loss:  0.06105085834860802
epoch:  2 loss:  0.02419847622513771
epoch:  2 loss:  0.032638318836688995
epoch:  2 loss:  0.007777002640068531
epoch:  2 loss:  0.005576186813414097
epoch:  2 loss:  0.010095223784446716
epoch:  2 loss:  0.004641587845981121
epoch:  2 loss:  0.0028811171650886536
epoch:  2 loss:  0.004671784117817879
epoch:  2 loss:  0.04335646331310272
epoch:  2 loss:  0.005388111807405949
epoch:  2 loss:  0.00341508025303483
epoch:  2 loss:  0.06975655257701874
epoch:  2 loss:  0.08710125833749771
epoch:  2 loss:  0.0044453744776546955
epoch:  2 loss:  0.03452563285827637
epoch:  2 loss:  0.003030648920685053
epoch:  2 loss:  0.006131869275122881
epoch:  2 loss:  0.041038066148757935
epoch:  2 loss:  0.010831114836037159
epoch:  2 loss:  0.1550237089395523
epoch:  2 loss:  0.01131113339215517
epoch:  2 loss:  0.006483559496700764
epoch:  2 loss:  0.004197025205940008
epoch:  2 loss:  0.016127659007906914
epoch:  2 loss:  0.010895125567913055
epoch:  2 loss:  0.008039030246436596
epoch:  2 loss:  0.15789084136486053
epoch:  2 loss:  0.014000123366713524
epoch:  2 loss:  0.017786730080842972
epoch:  2 loss:  0.012380171567201614
epoch:  2 loss:  0.0036549842916429043
epoch:  2 loss:  0.07959984987974167
epoch:  2 loss:  0.0049753314815461636
epoch:  2 loss:  0.005933853331953287
epoch:  2 loss:  0.0064429305493831635
epoch:  2 loss:  0.002709128661081195
epoch:  2 loss:  0.06468715518712997
epoch:  2 loss:  0.010381669737398624
epoch:  2 loss:  0.005289230961352587
epoch:  2 loss:  0.01555555034428835
epoch:  2 loss:  0.004388096742331982
epoch:  2 loss:  0.0028067696839571
epoch:  2 loss:  0.07570521533489227
test Accuracy: 0.9487680288461539
